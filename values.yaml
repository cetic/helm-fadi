---
# Default values for FADI.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
spark:
  enabled: true
  Master:
    Name: spark-master
  WebUi:
    Name: spark-webui
  Worker:
    Name: spark-worker
  Zeppelin:
    Name: spark-zeppelin
    Replicas: 0

superset:
  enabled: true
  persistence:
    enabled: true
  service:
    type: NodePort
  ingress:
    enabled: true
    annotations: {}
    path: /
    hosts: [superset.fadi.minikube]
  configFile: |-

    from flask_appbuilder.security.manager import AUTH_DB,AUTH_LDAP

    #---------------------------------------------------------
    # Superset specific config
    #---------------------------------------------------------

    ROW_LIMIT = 5000
    SUPERSET_WORKERS = 2
    SUPERSET_WEBSERVER_PORT = 8088
    #---------------------------------------------------------
    #---------------------------------------------------------
    # Flask App Builder configuration
    #---------------------------------------------------------
    # Your App secret key
    SECRET_KEY = '\2\1thisismyscretkey\1\2\e\y\y\h'
    # The SQLAlchemy connection string to your database backend
    # This connection defines the path to the database that stores your
    # superset metadata (slices, connections, tables, dashboards, ...).
    # Note that the connection information to connect to the datasources
    # you want to explore are managed directly in the web UI
    SQLALCHEMY_DATABASE_URI = 'sqlite:////var/lib/superset/superset.db'
    # Flask-WTF flag for CSRF
    WTF_CSRF_ENABLED = True
    # Add endpoints that need to be exempt from CSRF protection
    WTF_CSRF_EXEMPT_LIST = []
    # Set this API key to enable Mapbox visualizations
    MAPBOX_API_KEY = ''

    DEBUG=True
    LOG_FORMAT = '%(asctime)s:%(levelname)s:%(name)s:%(message)s'
    LOG_LEVEL = 'DEBUG'

    AUTH_TYPE = AUTH_LDAP
    AUTH_LDAP_SERVER = "ldap://fadi-openldap:389"
    AUTH_LDAP_USE_TLS = False
    AUTH_USER_REGISTRATION = True
    AUTH_LDAP_SEARCH = "dc=ldap,dc=cetic,dc=be"
    AUTH_LDAP_BIND_USER = "cn=admin,dc=ldap,dc=cetic,dc=be"
    AUTH_LDAP_BIND_PASSWORD = "password1"
    AUTH_LDAP_UID_FIELD = "cn"

postgresql:
  enabled: true
  persistence:
    enabled: true
  ldap:
    enabled: true
    pgldapconfig: |-
      # Reference: https://github.com/larskanis/pg-ldap-sync/blob/master/config/sample-config.yaml
      # Connection parameters to LDAP server
      ldap_connection:
        host: fadi-openldap
        port: 389
        auth:
          method: :simple
          username: CN=admin,DC=ldap,DC=cetic,DC=be
          password: password1
        # Search parameters for LDAP users which should be synchronized
      ldap_users:
        base: CN=admin,DC=ldap,DC=cetic,DC=be
        # LDAP filter (according to RFC 2254)
        # defines to users in LDAP to be synchronized
        filter: (!(cn=admin))
        # this attribute is used as PG role name
        name_attribute: cn
        # lowercase name for use as PG role name
        lowercase_name: true
      ldap_groups:
          base: DC=ldap,DC=cetic,DC=be
          filter: (|(cn=group1)(cn=group2)(cn=group3))
          # this attribute is used as PG role name
          name_attribute: cn
          # this attribute must reference to all member DN's of the given group
          member_attribute: member
      # Connection parameters to PostgreSQL server
      # see also: http://rubydoc.info/gems/pg/PG/Connection#initialize-instance_method
      pg_connection:
        host: fadi-postgresql
        dbname: postgres # the db name is usually "postgres"
        user: admin # the user name is usually "postgres"
        password: password1 # kubectl get secret --namespace fadi <pod_name> -o jsonpath="{.data.postgresql-password}" | base64 --decode
      pg_users:
        # Filter for identifying LDAP generated users in the database.
        # It's the WHERE-condition to "SELECT rolname, oid FROM pg_roles"
        # filter: rolcanlogin AND NOT rolsuper
        filter: oid IN (SELECT pam.member FROM pg_auth_members pam JOIN pg_roles pr ON pr.oid=pam.roleid WHERE pr.rolname='ldap_users')
        # Options for CREATE RULE statements
        create_options: LOGIN IN ROLE ldap_users
      pg_groups:
        # Filter for identifying LDAP generated groups in the database.
        # It's the WHERE-condition to "SELECT rolname, oid FROM pg_roles"
        # filter: NOT rolcanlogin AND NOT rolsuper
        filter: oid IN (SELECT pam.member FROM pg_auth_members pam JOIN pg_roles pr ON pr.oid=pam.roleid WHERE pr.rolname='ldap_groups')
        # Options for CREATE RULE statements
        create_options: NOLOGIN IN ROLE ldap_groups
        #grant_options:
    cron:
      schedule: "*/1 * * * *"
      repo: ceticasbl/pg-ldap-sync
      tag: latest
      restartPolicy: Never
      mountPath: /workspace
      subPath: ""
  postgresql:
    username: admin
    password: password1
    database: postgres
    pghba: |-
      local all all ldap ldapserver=fadi-openldap  ldapport=389 ldaptls=0 ldapbasedn="dc=ldap,dc=cetic,dc=be" ldapbinddn="cn=admin,dc=ldap,dc=cetic,dc=be" ldapbindpasswd=password1  ldapsearchfilter=cn=$username
      host all all 0.0.0.0/0  ldap ldapserver=fadi-openldap  ldapport=389 ldaptls=0 ldapbasedn="dc=ldap,dc=cetic,dc=be" ldapbinddn="cn=admin,dc=ldap,dc=cetic,dc=be" ldapbindpasswd=password1  ldapsearchfilter=cn=$username
    initdbscripts: |-
      #!/bin/sh
      psql -c "create role ldap_users;" postgres admin
      psql -c "create role ldap_groups;" postgres admin
minio:
  enabled: true
  persistence:
    enabled: true
  service:
    type: NodePort
  ingress:
    enabled: true
    annotations: {}
    path: /
    hosts: [minio.fadi.minikube]

grafana:
  enabled: true
  service:
    type: NodePort
  ingress:
    enabled: true
    annotations: {}
    path: /
    hosts: [grafana.fadi.minikube]
  grafana.ini:
    paths:
      data: /var/lib/grafana/data
      logs: /var/log/grafana
      plugins: /var/lib/grafana/plugins
      provisioning: /etc/grafana/provisioning
    analytics:
      check_for_updates: true
    log:
      mode: console
    grafana_net:
      url: https://grafana.net
    ## LDAP Authentication can be enabled with the following values on grafana.ini
    ## NOTE: Grafana will fail to start if the value for ldap.toml is invalid
    # ----- auth -------
    auth.ldap:
      enabled: true
      allow_sign_up: true
      config_file: /etc/grafana/ldap.toml
  # Enable persistence
  persistence:
    enabled: true
  # Administrator credentials when not using an existing secret (see below)
  adminUser: admin

  ## Grafana's LDAP configuration
  ## Templated by the template in _helpers.tpl
  ## NOTE: To enable the grafana.ini must be configured with auth.ldap.enabled
  ## ref: http://docs.grafana.org/installation/configuration/#auth-ldap
  ## ref: http://docs.grafana.org/installation/ldap/#configuration
  ldap:
    # `existingSecret` is a reference to an existing secret containing the ldap configuration
    # for Grafana in a key `ldap-toml`.
    existingSecret: ""
    # `config` is the content of `ldap.toml` that will be stored in the created secret
    config: |-
      verbose_logging = true

      [[servers]]

      host = "fadi-openldap"
      port = 389
      use_ssl = false
      start_tls = false
      ssl_skip_verify = false
      bind_dn = "cn=%s,DC=ldap,DC=cetic,DC=be"
      ##bind_password = 'password1'
      search_filter = "(cn=%s)"
      search_base_dns = ["DC=ldap,DC=cetic,DC=be"]

      [[servers.group_mappings]]

      group_dn = "*"
      org_role = "Admin"
      ##grafana_admin = true

   # ----- auth -----

jupyterhub:
  enabled: true
  proxy:
    secretToken: 'af83775ec3bfaf0507ce596df51d491e7ed54450adc454038fa7405495465f19'
    db:
      type: sqlite-memory
  rbac:
    enabled: true
  singleuser:
    storage:
      type: none
  # Defines the default image
    image:
      name: jupyter/minimal-notebook
      tag: 7d427e7a4dde
    profileList:
      - display_name: "Minimal environment"
        description: "To avoid too much bells and whistles: Python."
        default: true
      - display_name: "Datascience environment"
        description: "If you want the additional bells and whistles: Python, R, and Julia."
        kubespawner_override:
          image: jupyter/datascience-notebook:7d427e7a4dde
      - display_name: "Spark environment"
        description: "The Jupyter Stacks spark image"
        kubespawner_override:
          image: jupyter/all-spark-notebook:latest
  # ---- auth ----
  auth:
    type: ldap
    ldap:
      server:
        address: fadi-openldap
      dn:
        templates:
          - 'cn={username},cn=admin,dc=ldap,dc=cetic,dc=be'
          - 'uid={username},cn=admin,dc=ldap,dc=cetic,dc=be'
          - 'cn={username},dc=ldap,dc=cetic,dc=be'
  # ---- auth ----
  prePuller:
    hook:
      enabled: false
  ingress:
    enabled: true
    annotations: {}
    hosts: [jupyterhub.fadi.minikube]
    pathSuffix: ''

nifi:
  enabled: true
  service:
    loadBalancer:
      enabled: true
      type: NodePort
  ingress:
    enabled: true
    annotations: {}
    tls: []
    hosts: [nifi.fadi.minikube]
    path: /
  postStart: /opt/nifi/psql; wget -P /opt/nifi/psql https://jdbc.postgresql.org/download/postgresql-42.2.6.jar
  zookeeper:
    replicaCount: 1

pgadmin:
  enabled: true
  service:
    type: NodePort
  ingress:
    enabled: true
    annotations: {traefik.ingress.kubernetes.io/rule-type: Path}
    path: /
    hosts: [pgadmin.fadi.minikube]

openldap:
  enabled: true
  env:
    LDAP_ORGANISATION: "Cetic"
    LDAP_DOMAIN: "ldap.cetic.be"
    LDAP_BASE_DN: "cn=admin,dc=ldap,dc=cetic,dc=be"
    LDAP_BACKEND: "hdb"
    LDAP_TLS: "true"
    LDAP_TLS_ENFORCE: "false"
    LDAP_REMOVE_CONFIG_AFTER_SETUP: "false"
  adminPassword: password1
  configPassword: password2

  customLdifFiles:
    1-default-users.ldif: |-

      # You can find an example ldif file.

phpldapadmin:
  enabled: true
  service:
    type: NodePort
  env:
    PHPLDAPADMIN_LDAP_HOSTS: "fadi-openldap"
  ingress:
    enabled: true
    annotations: {}
    path: /
    hosts: [phpldapadmin.fadi.minikube]

kafka:
  enabled: false
  replicas: 1
  affinity: {}
  external:
    enabled: true
  persistence:
    enabled: false
  zookeeper:
    replicaCount: 1
    fullnameOverride: fadi-kafka-zookeeper

cassandra:
  enabled: false
  affinity: {}
  config:
    cluster_size: 1
    seed_size: 1
    start_rpc: true

elasticsearch:
  enabled: false

kibana:
  enabled: false
  env:
    ELASTICSEARCH_HOSTS: http://{{ .Release.Name }}-elasticsearch-client:9200

logstash:
  enabled: false
  elasticsearch:
    host: fadi-elasticsearch-client

filebeat:
  enabled: false
  config:
    output.file.enabled: false
    output.logstash:
      hosts: ["fadi-logstash:5044"]
  indexTemplateLoad:
   - fadi-elasticsearch-client:9200
