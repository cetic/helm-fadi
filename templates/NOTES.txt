Thank you for installing FADI!

{{- if .Values.grafana.enabled }}

#####      GRAFANA - FADI         #####
#######################################

The Grafana server can be accessed via port {{ .Values.grafana.service.port }} on the following DNS name from within your cluster:
   {{ template "grafana.fullname" . }}.{{ .Release.Namespace }}.svc.cluster.local
{{ if .Values.grafana.ingress.enabled }}
   From outside the cluster, the server URL(s) are:
{{- range .Values.grafana.ingress.hosts }}
     http://{{ . }}
{{- end }}
{{ else }}
   Get the Grafana URL to visit by running these commands in the same shell:
{{ if contains "NodePort" .Values.grafana.service.type -}}
     export NODE_PORT=$(kubectl get --namespace {{ .Release.Namespace }} -o jsonpath="{.spec.ports[0].nodePort}" services {{ template "grafana.fullname" . }})
     export NODE_IP=$(kubectl get nodes --namespace {{ .Release.Namespace }} -o jsonpath="{.items[0].status.addresses[0].address}")
     echo http://$NODE_IP:$NODE_PORT
{{ else if contains "LoadBalancer" .Values.grafana.service.type -}}
   NOTE: It may take a few minutes for the LoadBalancer IP to be available.
        You can watch the status of by running 'kubectl get svc --namespace {{ .Release.Namespace }} -w {{ template "grafana.fullname" . }}'
     export SERVICE_IP=$(kubectl get svc --namespace {{ .Release.Namespace }} {{ template "grafana.fullname" . }} -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
     http://$SERVICE_IP:{{ .Values.grafana.service.port -}}
{{ else if contains "ClusterIP"  .Values.grafana.service.type }}
     export POD_NAME=$(kubectl get pods --namespace {{ .Release.Namespace }} -l "app={{ template "grafana.name" . }},release={{ .Release.Name }}" -o jsonpath="{.items[0].metadata.name}")
     kubectl --namespace {{ .Release.Namespace }} port-forward $POD_NAME 3000
{{- end }}
{{- end }}

Login with the username: {{ .Values.grafana.adminUser }} and get your 'admin' user password by running:

   kubectl get secret --namespace default fadi -o jsonpath="{.data.admin-password}" | base64 --decode ; echo


{{- if not .Values.grafana.persistence.enabled }}
WARNING: Persistence is DISABLED !
{{- end }}
{{- end }}


{{- if .Values.superset.enabled }}

#####      SUPERSET - FADI       #####
######################################

Superset can be accessed via port {{ .Values.superset.service.port }} on the following DNS name from within your cluster:
{{ include "superset.fullname" . }}.{{ .Release.Namespace }}.svc.cluster.local

Initially you can login with username/password: admin/admin.

{{- if not .Values.superset.persistence.enabled }}
WARNING: Persistence is DISABLED !
{{- end }}
{{- end }}

{{- if .Values.pgadmin.enabled }}

#####      PGADMIN - FADI        #####
######################################

Get the pgAdmin URL by running these commands:
{{- if .Values.pgadmin.ingress.enabled }}
  You should be able to access your new pgAdmin installation through
  http{{ if $.Values.pgadmin.ingress.tls }}s{{ end }}://{{ if $.Values.pgadmin.ingress.host }}{{.Values.pgadmin.ingress.host}}{{else}}your-cluster-ip{{end}}{{ $.Values.pgadmin.ingress.path }}
  {{if not $.Values.pgadmin.ingress.host}}

  Find out your cluster ip address by running:
  $ kubectl cluster-info
  {{ end }}

{{- else if contains "NodePort" .Values.pgadmin.service.type }}
  export NODE_PORT=$(kubectl get --namespace {{ .Release.Namespace }} -o jsonpath="{.spec.ports[0].nodePort}" services {{ template "pgadmin.fullname" . }})
  export NODE_IP=$(kubectl get nodes --namespace {{ .Release.Namespace }} -o jsonpath="{.items[0].status.addresses[0].address}")
  echo "pgAdmin URL: http://$NODE_IP:$NODE_PORT"

{{- else if contains "LoadBalancer" .Values.pgadmin.service.type }}
  NOTE: It may take a few minutes for the LoadBalancer IP to be available.
         You can watch the status of by running 'kubectl get svc -w {{ template "pgadmin.fullname" . }}'

  export SERVICE_IP=$(kubectl get svc --namespace {{ .Release.Namespace }} {{ template "pgadmin.fullname" . }} --template "{{"{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}"}}")
  echo "pgAdmin URL: http://$SERVICE_IP:{{ .Values.pgadmin.service.port }}"

{{- else if contains "ClusterIP" .Values.pgadmin.service.type }}
  export POD_NAME=$(kubectl get pods --namespace {{ .Release.Namespace }} -l "app={{ template "pgadmin.name" . }},release={{ .Release.Name }}" -o jsonpath="{.items[0].metadata.name}")
  echo "pgAdmin URL: http://127.0.0.1:8080"
  kubectl port-forward --namespace {{ .Release.Namespace }} svc/{{ template "pgadmin.fullname" . }} 8080:{{ .Values.pgadmin.service.port }}
{{- end }}
{{- end }}


{{- if .Values.minio.enabled }}

#####      MINIO - FADI          #####
######################################

{{- if eq .Values.minio.service.type "ClusterIP" "NodePort" }}
Minio can be accessed via port {{ .Values.minio.service.port }} on the following DNS name from within your cluster:
{{ template "minio.fullname" . }}.{{ .Release.Namespace }}.svc.cluster.local

To access Minio from localhost, run the below commands:

  1. export POD_NAME=$(kubectl get pods --namespace {{ .Release.Namespace }} -l "release={{ .Release.Name }}" -o jsonpath="{.items[0].metadata.name}")

  2. kubectl port-forward $POD_NAME 9000 --namespace {{ .Release.Namespace }}

Read more about port forwarding here: http://kubernetes.io/docs/user-guide/kubectl/kubectl_port-forward/

You can now access Minio server on http://localhost:9000. Follow the below steps to connect to Minio server with mc client:

  1. Download the Minio mc client - https://docs.minio.io/docs/minio-client-quickstart-guide

  2. mc config host add {{ template "minio.fullname" . }}-local http://localhost:9000 {{ .Values.minio.accessKey }} {{ .Values.minio.secretKey }} S3v4

  3. mc ls {{ template "minio.fullname" . }}-local

Alternately, you can use your browser or the Minio SDK to access the server - https://docs.minio.io/categories/17
{{- end }}
{{- if eq .Values.minio.service.type "LoadBalancer" }}
Minio can be accessed via port {{ .Values.minio.service.port }} on an external IP address. Get the service external IP address by:
kubectl get svc --namespace {{ .Release.Namespace }} -l app={{ template "minio.fullname" . }}

Note that the public IP may take a couple of minutes to be available.

You can now access Minio server on http://<External-IP>:9000. Follow the below steps to connect to Minio server with mc client:

  1. Download the Minio mc client - https://docs.minio.io/docs/minio-client-quickstart-guide

  2. mc config host add {{ template "minio.fullname" . }}-local http://<External-IP>:{{ .Values.minio.service.port }} {{ .Values.minio.accessKey }} {{ .Values.minio.secretKey }} S3v4

  3. mc ls {{ template "minio.fullname" . }}-local

Alternately, you can use your browser or the Minio SDK to access the server - https://docs.minio.io/categories/17
{{- end }}

{{ if and (.Values.minio.networkPolicy.enabled) (not .Values.minio.networkPolicy.allowExternal) }}
Note: Since NetworkPolicy is enabled, only pods with label
{{ template "minio.fullname" . }}-client=true"
will be able to connect to this minio cluster.
{{- end }}

{{- end }}

{{- if .Values.jupyterhub.enabled }}
#####   JUPYTERHUB - FADI      #####
####################################

Your release is named {{ .Release.Name }} and installed into the namespace {{ .Release.Namespace }}.

You can find if the hub and proxy is ready by doing:

 kubectl --namespace={{ .Release.Namespace }} get pod

and watching for both those pods to be in status 'Running'.

You can find the public IP of the JupyterHub by doing:

 kubectl --namespace={{ .Release.Namespace }} get svc proxy-public

It might take a few minutes for it to appear!

Note that this is still an alpha release! If you have questions, feel free to
  1. Read the guide at https://z2jh.jupyter.org
  2. Chat with us at https://gitter.im/jupyterhub/jupyterhub
  3. File issues at https://github.com/jupyterhub/zero-to-jupyterhub-k8s/issues

{{- if .Values.jupyterhub.hub.extraConfigMap }}

DEPRECATION: hub.extraConfigMap is deprecated in jupyterhub chart 0.8.
Use top-level `custom` instead:

---
custom:
{{- (merge dict .Values.jupyterhub.custom .Values.hub.extraConfigMap) | toYaml | nindent 2}}
---
{{- end }}

{{- if and (not .Values.jupyterhub.scheduling.podPriority.enabled) (and .Values.jupyterhub.scheduling.userPlaceholder.enabled .Values.jupyterhub.scheduling.userPlaceholder.replicas) }}

WARNING: You are using user placeholders without pod priority enabled, either
enable pod priority or stop using the user placeholders to avoid wasting cloud
resources.
{{- end }}

{{- end }}

{{- if .Values.spark.enabled }}

#####      SPARK - FADI        #####
####################################

Get the Spark URL to visit by running these commands in the same shell:
  
  NOTE: It may take a few minutes for the LoadBalancer IP to be available.
  You can watch the status of by running 'kubectl get svc --namespace {{ .Release.Namespace }} -w {{ template "name" . }}'
  
  export SPARK_SERVICE_IP=$(kubectl get svc --namespace {{ .Release.Namespace }} {{ template "name" . }} -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
  echo http://$SPARK_SERVICE_IP:{{ .Values.spark.WebUi.ServicePort }}

Get the Zeppelin URL to visit by running these commands in the same shell:
  
  NOTE: It may take a few minutes for the LoadBalancer IP to be available.
  You can watch the status of by running 'kubectl get svc --namespace {{ .Release.Namespace }} -w {{ template "name" . }}'

  export ZEPPELIN_SERVICE_IP=$(kubectl get svc --namespace {{ .Release.Namespace }} {{ template "name" . }} -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
echo http://$ZEPPELIN_SERVICE_IP:{{ .Values.spark.Zeppelin.ServicePort }}

{{- end }}

{{- if .Values.postgresql.enabled }}

#####    POSTGRESQL - FADI     #####
####################################


** Please be patient while the chart is being deployed **

PostgreSQL can be accessed via port {{ .Values.postgresql.postgresql.port }} on the following DNS name from within your cluster:

    {{ template "postgresql.fullname" . }}.{{ .Release.Namespace }}.svc.cluster.local - Read/Write connection

To get the password for "{{ .Values.postgresql.postgresql.username  }}" run:

    export POSTGRES_PASSWORD=$(kubectl get secret --namespace {{ .Release.Namespace }} {{ if .Values.postgresql.existingSecret }}{{ .Values.postgresql.existingSecret }}{{ else }}{{ template "postgresql.fullname" . }}{{ end }} -o jsonpath="{.data.postgresql-password}" | base64 --decode)

To connect to your database run the following command:

    kubectl run {{ template "postgresql.fullname" . }}-client --rm --tty -i --restart='Never' --namespace {{ .Release.Namespace }} --image {{ .Values.postgresql.image.repository }} --env="PGPASSWORD=$POSTGRES_PASSWORD" --command -- psql --host {{ template "postgresql.fullname" . }} -U {{ .Values.postgresql.postgresql.username }}{{- if .Values.postgresql.postgresql.database }} -d {{ .Values.postgresql.postgresql.database }}{{- end }} -p {{ .Values.postgresql.postgresql.port }}

To connect to your database from outside the cluster execute the following commands:

{{- if contains "NodePort" .Values.postgresql.service.type }}

    export NODE_IP=$(kubectl get nodes --namespace {{ .Release.Namespace }} -o jsonpath="{.items[0].status.addresses[0].address}")
    export NODE_PORT=$(kubectl get --namespace {{ .Release.Namespace }} -o jsonpath="{.spec.ports[0].nodePort}" services {{ template "postgresql.fullname" . }})
    {{ if (.Values.postgresql.postgresql.password)  }}PGPASSWORD="$POSTGRES_PASSWORD" {{ end }}psql --host $NODE_IP --port $NODE_PORT -U {{ .Values.postgresql.postgresql.username }}{{- if .Values.postgresql.postgresql.database }} -d {{ .Values.postgresql.postgresql.database }}{{- end }}

{{- else if contains "LoadBalancer" .Values.postgresql.service.type }}

  NOTE: It may take a few minutes for the LoadBalancer IP to be available.
        Watch the status with: 'kubectl get svc --namespace {{ .Release.Namespace }} -w {{ template "postgresql.fullname" . }}'

    export SERVICE_IP=$(kubectl get svc --namespace {{ .Release.Namespace }} {{ template "postgresql.fullname" . }} --template "{{"{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}"}}")
    {{ if (.Values.postgresql.postgresql.password) }}PGPASSWORD="$POSTGRES_PASSWORD" {{ end }}psql --host $SERVICE_IP --port {{ .Values.postgresql.postgresql.port }} -U {{ .Values.postgresql.postgresql.username }}{{- if .Values.postgresql.postgresql.database }} -d {{ .Values.postgresql.postgresql.database }}{{- end }}

{{- else if contains "ClusterIP" .Values.postgresql.service.type }}

    kubectl port-forward --namespace {{ .Release.Namespace }} svc/{{ template "postgresql.fullname" . }} {{ .Values.postgresql.postgresql.port }}:{{ .Values.postgresql.postgresql.port }} &
    {{ if (.Values.postgresql.postgresql.password) }}PGPASSWORD="$POSTGRES_PASSWORD" {{ end }}psql --host 127.0.0.1 -U {{ .Values.postgresql.postgresql.username  }}{{- if .Values.postgresql.postgresql.database }} -d {{ .Values.postgresql.postgresql.database  }}{{- end }} -p {{ .Values.postgresql.postgresql.port}}

{{- end }}

{{- end }}

{{- if .Values.openldap.enabled }}

#####      Openldap - FADI         #####
#######################################

OpenLDAP has been installed. You can access the server from within the k8s cluster using:

  {{ template "openldap.fullname" . }}.{{ .Release.Namespace }}.svc.cluster.local:{{ .Values.openldap.service.ldapPort }}


You can access the LDAP adminPassword and configPassword using:

  kubectl get secret --namespace {{ .Release.Namespace }} {{ template "openldap.secretName" . }} -o jsonpath="{.data.LDAP_ADMIN_PASSWORD}" | base64 --decode; echo
  kubectl get secret --namespace {{ .Release.Namespace }} {{ template "openldap.secretName" . }} -o jsonpath="{.data.LDAP_CONFIG_PASSWORD}" | base64 --decode; echo


You can access the LDAP service, from within the cluster (or with kubectl port-forward) with a command like (replace password and domain):
  ldapsearch -x -H ldap://{{ template "openldap.fullname" . }}-service.{{ .Release.Namespace }}.svc.cluster.local:{{ .Values.openldap.service.ldapPort }} -b dc=example,dc=org -D "cn=admin,dc=example,dc=org" -w $LDAP_ADMIN_PASSWORD


Test server health using Helm test:
  helm test {{ .Release.Name }}

{{- end }}

{{- if .Values.phpldapadmin.enabled }}

#####      phpLDAPadmin - FADI         #####
#######################################

1. Get the application URL by running these commands:
{{- if .Values.phpldapadmin.ingress.enabled }}
{{- range .Values.phpldapadmin.ingress.hosts }}
  http{{ if $.Values.phpldapadmin.ingress.tls }}s{{ end }}://{{ . }}{{ $.Values.phpldapadmin.ingress.path }}
{{- end }}
{{- else if contains "NodePort" .Values.phpldapadmin.service.type }}
  export NODE_PORT=$(kubectl get --namespace {{ .Release.Namespace }} -o jsonpath="{.spec.ports[0].nodePort}" services {{ template "phpldapadmin.fullname" . }})
  export NODE_IP=$(kubectl get nodes --namespace {{ .Release.Namespace }} -o jsonpath="{.items[0].status.addresses[0].address}")
  echo http://$NODE_IP:$NODE_PORT
{{- else if contains "LoadBalancer" .Values.phpldapadmin.service.type }}
     NOTE: It may take a few minutes for the LoadBalancer IP to be available.
           You can watch the status of by running 'kubectl get svc -w {{ template "phpldapadmin.fullname" . }}'
  export SERVICE_IP=$(kubectl get svc --namespace {{ .Release.Namespace }} {{ template "phpldapadmin.fullname" . }} -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
  echo http://$SERVICE_IP:{{ .Values.phpldapadmin.service.port }}
{{- else if contains "ClusterIP" .Values.phpldapadmin.service.type }}
  export POD_NAME=$(kubectl get pods --namespace {{ .Release.Namespace }} -l "app={{ template "phpldapadmin.name" . }},release={{ .Release.Name }}" -o jsonpath="{.items[0].metadata.name}")
  echo "Visit http://127.0.0.1:8080 to use your application"
  kubectl port-forward $POD_NAME 8080:80
{{- end }}
          
{{- end }}

{{- if .Values.filebeat.enabled }}

#####      Filebeat - FADI         #####
#######################################


******* Filebeat will work once all the Elasticsearch pods are running, it might take a while. *********

To verify that Filebeat has started, run:

  kubectl --namespace={{ .Release.Namespace }} get pods -l "app={{ template "filebeat.name" . }},release={{ .Release.Name }}"


{{- end }}


{{- if .Values.logstash.enabled }}

#####      logstash - FADI         #####
#######################################

{{- if .Values.logstash.service.ports.http }}
Get the Logstash HTTP Input URL by running these commands:
  {{- if .Values.logstash.ingress.enabled }}
    {{- range .Values.logstash.ingress.hosts }}
  http{{ if $.Values.logstash.ingress.tls }}s{{ end }}://{{ . }}{{ $.Values.logstash.ingress.path }}
    {{- end }}
  {{- else if contains "NodePort" .Values.logstash.service.type }}
  export NODE_PORT=$(kubectl get --namespace {{ .Release.Namespace }} -o jsonpath="{.spec.ports[0].nodePort}" services {{ template "logstash.fullname" . }})
  export NODE_IP=$(kubectl get nodes --namespace {{ .Release.Namespace }} -o jsonpath="{.items[0].status.addresses[0].address}")
  echo http://$NODE_IP:$NODE_PORT
  {{- else if contains "LoadBalancer" .Values.logstash.service.type }}
     NOTE: It may take a few minutes for the LoadBalancer IP to be available.
           You can watch the status of by running 'kubectl get svc -w {{ template "logstash.fullname" . }}'
  export SERVICE_IP=$(kubectl get svc --namespace {{ .Release.Namespace }} {{ template "logstash.fullname" . }} -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
  echo http://$SERVICE_IP:{{ .Values.logstash.service.ports.http.port }}
  {{- else if contains "ClusterIP" .Values.logstash.service.type }}
  export POD_NAME=$(kubectl get pods --namespace {{ .Release.Namespace }} -l "app={{ template "logstash.name" . }},release={{ .Release.Name }}" -o jsonpath="{.items[0].metadata.name}")
  echo "Visit http://127.0.0.1:8080 to use your application"
  kubectl port-forward $POD_NAME 8080:{{ .Values.logstash.service.ports.http.port }}
  {{- end }}
{{- end }}

{{- end }}

{{- if .Values.elasticsearch.enabled }}

#####      elasticsearch - FADI         #####
#######################################

The elasticsearch cluster has been installed.

     WARNING: You have likely exposed your Elasticsearch cluster direct to the internet.
              Elasticsearch does not implement any security for public facing clusters by default.
              As a minimum level of security;  place an Nginx gateway infront of the cluster in order to lock down access to dangerous HTTP endpoints and verbs.

{{- end }}


{{- if .Values.kibana.enabled }}

#####      kibana - FADI         #####
#######################################

To verify that {{ template "kibana.fullname" . }} has started, run:

  kubectl --namespace={{ .Release.Namespace }} get pods -l "app={{ template "kibana.name" . }}"

Kibana can be accessed:

  * From outside the cluster, run these commands in the same shell:
    {{- if contains "NodePort" .Values.kibana.service.type }}

    export NODE_PORT=$(kubectl get --namespace {{ .Release.Namespace }} -o jsonpath="{.spec.ports[0].nodePort}" services {{ template "kibana.fullname" . }})
    export NODE_IP=$(kubectl get nodes --namespace {{ .Release.Namespace }} -o jsonpath="{.items[0].status.addresses[0].address}")
    echo http://$NODE_IP:$NODE_PORT
    {{- else if contains "ClusterIP"  .Values.kibana.service.type }}

    export POD_NAME=$(kubectl get pods --namespace {{ .Release.Namespace }} -l "app={{ template "kibana.name" . }},release={{ .Release.Name }}" -o jsonpath="{.items[0].metadata.name}")
    echo "Visit http://127.0.0.1:5601 to use Kibana"
    kubectl port-forward --namespace {{ .Release.Namespace }} $POD_NAME 5601:5601
    {{- end }}

{{- end }}


CONGRATULATIONS! FADI is being installed.